% Template for ICASSP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,bm,setspace}
\usepackage{todonotes}
\setlength{\marginparwidth}{1.5cm}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{threeparttable}

% ADD THE FOLLOWING COUPLE LINES INTO YOUR PREAMBLE
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Transfer of knowledge between concepts: An application to sincerity and deception prediction}
%
% Single address.
% ---------------
\name{Qinyi Luo$^+$, Rahul Gupta$^o$, Shrikanth Narayanan$^o$}
\address{ $^+$Tsingua University, Beijing, China \\
$^o$Signal Analysis and Interpretation Lab, University of Southern California,  Los Angeles, CA, USA}  
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
   Put abstract here
 
\end{abstract}
%
\begin{keywords}
Pathological speech disorders, machine learning, signal processing 
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Put intro here

\section{Related work}
Put background here

\section{Datasets}
%Two databases from the INTERSPEECH 2016 Computational Paralinguistics Challenge are employed in the experiments, the Deceptive Speech Database (DSD) and the Sincerity Speech Corpus (SSC). For both databases, samples outside of the test set are accessible, constituting the datasets for our experiments, denoted as the D dataset and the S dataset respectively. For both datasets, the official ComParE Acoustic Feature Set was used, containing 6373 static features computed over low-level descriptor contours. For more information about the Challenge, please refer to [x].\par
%Descriptions of the two datasets are detailed below.

%\subsection{D dataset}
%The DSD was produced at the University of Arizona, consisting of 1556 speech samples from 72 speakers recorded in structured interviews. A total of 1059 samples are contained in the D dataset. For each sample, the label is either Deceptive (D) or Non-Deception (ND). In the Deceptive condition, participants were asked to lie about their identity and behavior, while in the Non-Deceptive condition, they were asked to tell the truth. The interviews were carried out by an Embodied Conversational Agent (ECA) in order to ensure consistency.

%\subsection{S dataset}
%The SSC provided by the Columbia University contain 655 available speech instances from 22 speakers and 256 instances held confidential by the Challenge. All of the 655 available instances were included in the S dataset. When recording the instances, a number of participants were asked to read six different sentences, each sentence in 4 different styles (monotonic, non-monotonic, fast and slow). Each instance was graded from 0 to 4 by at least 13 human annotators regarding perceived sincerity, with higher scores indicating more sincerity. Scores given by the same annotator were normalized to zero mean and unit standard deviation. Averaging the normalized scores from all annotators, a sincerity rating was obtained for each instance.

%\subsection{Comparisons between datasets D and S}
%The D dataset and the S dataset are linked together by the complementary relationship between deception and sincerity. Certain similarities are expected to exist between Deceptive samples and instances in the S dataset with low sincerity scores, and also between Non-deceptive samples and highly sincere instances.\par
%Nevertheless, in terms of recording conditions, the two datasets are hardly alike. 
Speech samples in the D dataset are more similar to natural speech, as the subjects were allowed to arrange speech and show emotions freely. On the contrary, participants for the S dataset were subjected to more restrictions, resulting in more acting. Moreover, labels for the D dataset are objective and binary, while sincerity ratings are subjective and continuous.\par
%The aforementioned differences pose challenge to transferring knowledge from one dataset to the other. On the other hand, they help simulate a real-world inter-concept knowledge transfer situation, where little consistency is guaranteed between corpora.

We use two datasets for the purpose of our experiments: (i) deception dataset and, (ii) sincerity dataset.
These datasets were also used as part of the Interspeech 2016 computational paralinguistic challenge.
We breifly describe these two datasets below.\\ 

\noindent{\bf Deceptive Speech Dataset (DSD)}:
The DSD dataset is available from the University of Arizona and we use a set of 1059 speech samples obtained from x participants.
In each of these samples, the participant either lies about his identity and behavior or tells the truth. 
Therefore, each of these samples is associated with a binary label of the speech being deceptive or not.`
We refer the reader to \cite{} for further details on this dataset.\\ 

\noindent{\bf Sincerity Speech Corpus (SSC)}:
We use the SSC dataset provided by the Columbia Univesity, consisting of 655 speech samples from 22 speakers.
Unlike the labelling schemes for the DSD dataset, the samples in the SSC dataset are rated for percieved sincerity by a group of 13 annotators in a range of 0-4. 
The scores for each annotator were further normalized to zero mean and unit standard deviation.
Final sincerity score for each sample is computed as the mean of thus normalized sincerity score from each annotator.
Further details regarding the dataset are available in \cite{}.

The goal of our experiments is to utilize the expected relationship between deception and perceived sincerity lables.
We hypothesize the deceptive utterances would have a low percieved sincerity, and also, utterances with low percieved sincerity are more likely to be decptive.
Therefore, in order to improve the performance for one dataset, we aim to utilize the knowledge from the other dataset by either directly using the datasamples from that dataset or using the model learnt on the other dataset.
However, we need to account for several dissimilarities between the datasets for a maximal transfer of knowledge from one dataset to the others.
These dissimilarities exist in the form of differences in dataset collection conditions and protocols, annotation procedures and the nature of lables.
Speech samples in the DSD dataset are more similar to natural speech, as the subjects were allowed to arrange speech and show emotions freely. 
On the contrary, participants for the SSC dataset were asked to utter pre-specified sentences. 
Moreover, labels for the DSD dataset are objective and binary, while sincerity ratings in the SSC dataset are subjective and continuous.
We address these issues using a few label generation and data transformation techniques.
These label generation and data transformation operate on on features extracted from each of the two dataset, as described next.

\subsection{Features} 
We use a set of 6376 acoustic features termed as the ComParE Acoustic Feature Set \cite{}.
These features are statistics computed on prosodic cues (e.g. pitch, intensity, jitter and, shimmer) and spectral cues (Mel Frequency Cepstral Coefficients xxx..).
Further details on these features are listed in Table x in \cite{}. 


\section{Methodology}
%In the experiments, we first determined the baselines by training and testing on the two datasets separately. Then we transferred knowledge between the two datasets, during which process data information as well as label information of one dataset can be utilized to enhance affect computing on the other dataset. We experimented with 3 knowledge transfer methods. The first method was the Random Sample Consensus (RANSAC) algorithm, which utilized data information in the transfer, but ignored label information. The second method tried to utilize both data and label information via label transformation. Compared to these 2 methods, we proposed a novel method that exploited both data and label information and produced better results.\par
%For classification on the D dataset, unweighted average recall (UAR) was measured. For regression on the sincerity ratings, Spearman's correlation was computed.
%For ease of exposition, in the rest of this paper, premodifier 'D' indicates relation to the D dataset or deception identification, while premodifier 'S' implies relation to the S dataset or sincerity evaluation.

The goal of experiments is to exploit the information from one dataset for improved performance of target prediction on the other dataset.
For the purpose of this demonstration, we initially set a baseline on each of the two datasets.
We then conduct two categories of experiments to utilize the information from a given dataset in improving performance for the other dataset. 
In the first set of experiments, while predicting the target labels for the dataset at hand, we append the datapoints from the other dataset with labels of interest generated synthetically. 
In the second experiment, we use the outputs from a sincerity prediction model as features during deception prediction and vice versa.
Finally, we combine the two schemes of appending data and using predictions from the other model.
Below, we describe each of these modeling schemes in detail. 

\subsection{Baseline experiments}
%For deception detection, a classifier was trained on the D dataset using SVM classification. Ten-fold cross validation was implemented, with 1 fold as the test set, 1 fold as the development set and the rest as the train set. Z-scores were taken on all features with the mean and standard deviation computed on the corresponding train set. We tuned the box constraint on the development set, and tested the results using a model trained on the concatenated train and development sets with the optimal box constraint.\par
%For sincerity evaluation, a regressor was trained on the S dataset using SVM regression. A leave-one-speaker-out cross-validation schema was adopted. In each fold, the test set contained one speaker's speech samples, while the train set had 9 speakers and the development set had 12. All labels were scaled to range from -1 to 1. Z-scores were taken on all features with the mean and standard deviation computed on the corresponding train set. The cost parameter was tuned on the development set. To obtain the test set performance, we trained a model on the concatenated train and development sets using the optimal cost parameter.

In the baseline systems, we only use the datapoints corresponding to each individual dataset, without any transfer of knowledge beween the SSC and DSD datasets.
We describe the choice of baseline modeling schemes for the deception and sincerity prediction below.
\\

\noindent{\bf Deception prediction}  
We train a Support Vector Machine (SVM) classifier on the Compare Acoustic Feature Set \cite{} to predict the binary label of an utterance being deceptive or not.
Same classifier was also used in the baseline presented in the Interspeech challenge 2016 \cite{}.
For evaluation purpose, we perform a 10 fold cross-validation, each fold containing an independent set of speakers.  
8 partitions are used as training set, 1 as development set and 1 as testing set.
SVM parameters such as kernel and box-constraint are tuned on the development set.
We use Unweighted Average Recall (UAR) as the evaluation metric on the DSD dataset, as was also the case during the Interspeech challenge 2016 \cite{}. 
The baseline results are listed in Table~\ref{}.
Note that the results are slightly different than the one presented in the Interspeech 2016 challenge paper \cite{} as their evaluation defined a different partitioning scheme for the development and the testing set.
\\

\noindent{\bf Sincerity predicition}
Since the sincerity predicition involves continuous lables, we train a Support Vector Regressor (SVR) with spearman correlation as the evaluation metric. 
Same regressor and evaluation metric were used during the Interspeech challenge 2016.
In this case, we perform a leave one out speaker crossvalidation due to smaller number of datasamples.
Apart from the speaker in the testing set, other speakers are roughly equally divided between the training and the development set.
The parameters for the SVR (kerner and box-constraint) are tuned on a development set. 
Table~\ref{} presents the baseline results.
The results are slightly different from the ones presented in the Interspeech 2016 challenge paper \cite{} due to difference in partition and the fact that during the challenge, the box-constraint parameter was tuned globally for each fold.
We, on the other hand, find the best parameters for each for indipendently based on the development set. 

\subsection{Knowledge Transfer (KT)}
Using Knowledge Transfer, we aim to leverage one dataset for an improved performance on the other dataset.
We propose two KT experiments, namely: (i) appending datapoints from the other dataset and, (ii) appending sincerity/deception predictions from the other model as features.
In the first approach, We expect that adding datapoints from the other dataset with synthetically created lables provides a lower generalization error (as has been discussed in \cite{}). 
The second approach aims to exploit the hypotheses that there exists a relation between the sincerity and deception lables, therefore one can be used a predictor for the other.
We describe these experiments in detail below. 

\subsubsection{KT: appending datapoints}
%Based on the baselines, the knowledge transfer experiments aimed to improve the performance on one dataset with the help of the other dataset. To allow fair comparison, some procedures remained unchanged: (a) when training D classifiers, ten-fold cross validation was implemented in the same way as in the baselines, and the box constraint was tuned on the development set; (b) when obtaining S regressors, leave-one-speaker-out cross validation was adopted as in the baselines, and the best cost parameter was chosen on the development set.

In this section, we append the datapoints from the other dataset in order to improve the performance for the task of interest.
First, we generate synthetic labels for the task at hand for each datapoint in the other dataset.
These datapoints and synthetic labels are appended with the datapoints and the lables of the original dataset for final model training.
We test two synthetic label generation algorithms: Rxxx (Ransac) and label transformation.
We discuss these algorithms in detail below.

(a){Rxxx (RANSAC)}
%One way to transfer knowledge is to add one dataset as unlabeled data to the other dataset in hope that more data points will generate better results. In light of this, we implemented the RANSAC algorithm.\par
%RANSAC is an iterative algorithm. Starting from a small set of data points (the initial dataset), it enlarges the dataset by adding consistent data points and renews the model in each iteration. In an extreme case of RANSAC, the unlabeled data can be added into the initial dataset all in one iteration. More information about RANSAC can be found in [x].\par
%In our experiments, one of the D and S datasets served as the initial dataset, and the other dataset was treated as unlabeled data to be added. Z-scores were taken on both datasets to reconcile possible differences in data characteristics that resulted from recording conditions.\par
%For classification on the D dataset, the D train set served as the initial dataset, on which the initial classifier was instantiated. The S dataset was the initial unlabeled dataset. In each iteration, the classifier predicted labels for the unlabeled dataset. Data points that were far away from the decision hyperplane were added into the initial dataset and removed from the unlabeled dataset. A renewed classifier was then trained on the enlarged dataset. When adding the data points in each iteration, three different criteria were applied: (a) data points that were the farthest 5 percent away from the decision hyperplane were added; (b) data points that were the farthest 10 percent away from the decision hyperplane were added; (c) all of the unlabeled data points were added. To stop the iteration, either one of the following two conditions were required: (a) the performance on the development set was stable; (b) the unlabeled dataset was empty. To test the results, we selected the classifier from the iteration in which the performance on the development set was the highest.\par
%For regression on the S dataset, the D dataset was gradually added into the S dataset. An initial regressor was trained on the S train set. In each iteration, the unlabeled D data points were grouped into several clusters using k-means clustering. The number of clusters was determined using the silhouette method. Then one cluster was selected to add into the initial dataset by the following procedure: (a) each cluster was added into the initial dataset respectively, and regressors were trained on the augmented initial datasets. Therefore each cluster resulted in one regressor; (b) The new regressors were used to predict labels for the corresponding initial dataset, and root mean square errors were computed; (c) The cluster with the smallest corresponding root mean square error was selected. The selected cluster was then added into the initial dataset and excluded from the unlabeled dataset. The stopping criteria were the same as for the classification on the D dataset: (a) the performance on the development set was stable, or (b) the unlabeled dataset was empty. We also tried the extreme case of RANSAC, where the D dataset was added into the S train set all in one iteration. Judging from the performance on the development set, the best regressor among all iterations was used to obtain results on the test set.

During the RANSAC \cite{} implementation, an initial model is trained on the available dataset with lables.
Then, the algorithm performs prediction on the datapoints without labels.
A fraction of these datapoints in then added to the existing dataset with labels and an updated model is trained with the additional labelled data.
Researchers have proposed several criterion for selecting the fraction of data (e.g. adding the datapoints farthest away from class boundaries). 
The procedure is repeated iteratively till a certain criterion is met (in our experiments, we perform the iterations till the performance on the held out development set is maximized).
Since the task objective is different for the DSD and the SSC corpus, we describe them seprately below. 

\noindent{\bf Deception prediction}  
For the purpose of deception prediction, synthetic labels are obtained on the entire SSC dataset.
We use the same train, test and development set split as in the baseline experiments.
Initially, an SVM model is obtained using the train partition of the DSD dataset and datapoints are added from the SSC dataset till the performance on the development partition of the DSD dataset is maximized.
In each iteration, datapoints farthest away from the class boundary are added (more details on this criterion is available in \cite{}).
We vary the proportion of SSC data added at each iteration from farthest 5\% to all of the data.

\noindent{\bf Sincerity prediction}  
The details of sincerity prediction are similar to deception prediction, except for the fact that the model we train is an SVR.
We implement the regression version of RANSAC algorithm as proposed in \cite{} and the train, test and development set partitions are kept same as the baseline experiments.
Table~\ref{} shows the results for the RANSAC algorithm implementation.

\subsubsection{Label transformation}
%In an attempt to incorporate label information when appending one dataset to the other, we transformed the labels to achieve compatibility between binary and continuous labels. Z-scores were taken on both datasets to reconcile possible differences in data characteristics that resulted from recording conditions.\par
%For classification on the D dataset, the continuous S labels were binarized, and the S dataset was appended to the D train set along with the new labels. Binarization was achieved by comparing sincerity ratings to a threshold $V_{th}$, i.e. samples whose sincerity ratings were lower than $V_{th}$ were labeled as Deceptive while others were labeled non-deceptive. A classifier was trained on the enlarged train set, and the threshold was tuned on the development set. Using the optimal threshold and the optimal box constraint, we obtained the test set performance using a classifier trained on the concatenated train and development sets, which included the original train and development sets as well as the S dataset.\par
%For regression on the S dataset, we assigned different values to D and ND labels, i.e. all deceptive samples were given a sincerity rating $v_{1}$ while the non-deceptive samples were graded $v_{2}$. We enlarged the S train set by adding the D dataset, and tuned $v_{1}$ and $v_{2}$ on the development set. Results on the test set were tested using a regressor trained on the concatenated train and development sets.

Previously, we hypothesized that there exists a relation between the sincerity and the deception labels.
The RANSAC algorithm does not make of sincerity labels during training decption models (and vice versa). 
Alternatively during the label transformation, we transform the continous sincerity labels into binary deception labels and vice versa for deception and sincerity prediction, respectively.
We describe the label transformation for deception and sincerity prediction below.

\noindent{\bf Deception prediction}  
In this experiment, we append a part of the sincerity dataset to the DSD dataset durint model training.
The portion of the sincerity dataset with percieved sincerity below (/above) a certain threshold are marked as deceptive (/non-deceptive). 
These two threshold are tuned as a pair for maximal performance on the development partition of the DSD dataset. 

\noindent{\bf Sincerity prediction}
Obtaining sincerity labels on the deception dataset involves transforming binary labels into a continous sincerity scale.
For our experiments, we approximate all deceptive utterances to carry a fixed sincerity rating. 
Similarly, all non-decptive utterances are labeled with a different constant sincerity ratings.
These ratings for the deceptive and non-deceptive utterances are again tuned as a pair for maximal performance on the development partition of the SSC dataset.
The results for the label transformation are also listed in Table~\ref{}.

\subsection{Knowledge transfer: using sincerity/deception predictions as features}
In this section, we propose using sincerity prediction as features during deception prediction and vice versa.
This approach is motivated from our previous hypothesis that a relationship exists between deception and sincerity perception, therefore obtaining a sincerity score would be useful for deception prediction and vice versa.
%We adopt two different approaches to this effect.
In the experiment, we predict the sincerity scores (/deception prediction) on the DSD dataset to aid deception (/sincerity) prediction. 
These sincerity (/deception) scores are obtained from a model trained on an adapted version of the SSC (/DSD) dataset.
As there exists a mismatch between the SSC and DSD datasets, we perform a domain adaptation to reduce the differences in the distribution of SSC and DSD datasets.
We provide the experimental details for the experiments below.

%\subsubsection{Sincerity/deception prediction features obtained from original models}
%In this experiment, during deception (/sincerity) prediction we make use of sincerity (/deception) predictions obtained from models trained on the SSC (/DSD) dataset. We breifly explain the experiments for the deception and sincerity prediction below. 

\noindent{\bf Deception prediction} 
We use the train, test and development set partitions for the DSD dataset as described before.
The sincerity scores for the DSD dataset are obtained from a model trained on a domain adapted version of the SSC dataset.
We use the Geodesic Kernel Flow (GFK) adaptation \cite{} to this effect.
{\bf Check} We treat the SSC dataset as the source dataset and adapt it to have a similar distibution like the DSD dataset. 
We then train a model on the adapted SSC dataset and make predictions on the DSD dataset.
The sincerity scores are used in conjunction with the original acoustic features (in section~\ref{sec:}) to predict deception in a stacked generalized framework \cite{}.
In the stacked generalized framework a model is first trained to obtain predictions from the original features. 
These predictions are combined with the predicted sincerity scores on the DSD dataset by another SVM model to provide the final decision. 

\noindent{\bf Sincerity prediction}
Akin to the deception prediction experiments, we use the predefined train, test and development set partitions for the SSC dataset.
We predict the deception outcomes for the SSC dataset using a model trained on an adapted version of the DSD dataset.
The adaptation is again performed using GFK adaptation, with DSD dataset modified as source dataset to resemble SSC dataset as the target dataset.
An SVM classifier trained on the adapted version of the DSD dataset predicts deception outcomes on the SSC dataset. 
However, instead of using the binary deception lables, we use distance of datapoints from SVM decision hyperplanes as a soft deception score. 
We combine these deception scores with the original acoustic features again using a stacked generalization framework, where an initial prediction is first made just based on original features. 
Another SVR model then combines these initial predictions with the deception scores (distance from SVM hyperplanes) to provide the final sincerity scores.  

In the next section, we combined the appending datapoints and appending sincerity/deception outcomes as features. 

\subsection{Knowledge transfer: appending datapoints + using sincerity/deception predictions as features}
In this section, we combine the pre


%\subsubsection{Proposed method}
%The proposed method also exploited both data information and label information. However, unlike the aforementioned two methods, it implemented fusion of labels instead of combining the two datasets, and utilized a kernel-based domain adaptation method to reduce influence of differences in the recording conditions. In general, each data point was assigned 2 labels, one predicted by a D classifier and the other by a S regressor. The two labels were then combined to give a final decision.\par
%To predict labels for samples in one dataset using the training material of the same dataset, i.e. to predict labels for the D dataset using a D classifier and to predict labels for the S dataset using a S regressor, we made use of the classifiers and regressors from section 4.1, 4.2.1 and 4.2.2. The only difference was that the classifiers and regressors here were trained on their corresponding train sets only, while they were originally trained on the concatenated train and development sets. This was to ensure that the predictors here didn't see any data points in the development set, so it could still be reasonable to tune parameters for the fusion on the development set. The optimal box constraint and the optimal cost parameter were also obtained on the development set. In addition, in hope to obtain better performance, we tried to combine the aforementioned three methods by selecting the classifier/regressor that performed the best on the development set. After the prediction, for fusion of labels for the D dataset, distances from the decision hyperplane were used, because they contained more information than the binary labels. For S samples, we used the predicted sincerity ratings for fusion.\par
%To predict labels for samples in one dataset with a predictor trained on the other dataset, differences in data characteristics between the two datasets must be dealt with. This is because samples from one corpus can be very different from samples with the same degree of sincerity from the other corpus, which highly likely leads to wrong predictions. To ameliorate the differences, we performed unsupervised domain adaptation using Geodesic Flow Kernel (GFK). The GFK domain adaptation method aims to make datasets from different domains become more similar. It constructs a continuous transition from the source domain to the target domain, and along the way finds the best point for subspace projection, so that after the projection, low-dimensional domain-invariant representations of the two datasets can be derived. For more information, please refer to [x].\par
%After the transformation, a classifier was trained on the transformed D dataset to predict labels for the transformed S dataset, and we used the distance from the decision hyperplane for fusion as it contained more information. Similarly, a regressor was trained on the transformed S dataset to predict sincerity ratings for the transformed D dataset, and the labels were later used for fusion.\par
%For ease of notation, labels for one dataset that were predicted by a model trained on the same dataset are denoted as $Lbl_{orig}$, and those that were predicted by a model trained on the other dataset are denoted as $Lbl_{other}$.\par
%After the 2 labels were derived, they were both transformed into probability values by f(x) = exp(x)/[1+exp(x)]. Then different fusion strategies were adopted for D classification and S regression. For D classification, a combined decision was given by P = $\lambda$$P_{orig}$+(1-$\lambda$)$P_{other}$, where $P_{orig}$ and $P_{other}$ denoted the 2 probability values given by f(x). The parameter $\lambda$ was tuned on the development set, and the final binary labels were obtained by comparing P to 0.5. For S regression, a final decision was given by SVM regression on the 2 probability values. The cost parameter was tuned on the development set, and the test set performance was obtained using a model trained on the concatenated train and development sets.\par
%As a comparison, We tested another set of results using $Lbl_{other}$ alone (without transforming the labels into probability values). we also tried removing the GFK domain adaptation when testing with $Lbl_{other}$. Note that using $Lbl_{orig}$ alone would be the same as the methods in section 4.1, 4.2.1 and 4.2.2, therefore results in these three sections would automatically be a good comparison to the results of our proposed method.




\section{Results}
Generally speaking, the performance improved after the fusion of labels, no matter which classifier/regressor was used to derive $Lbl_{orig}$. It did matter, however, whether the GFK domain adaptation was used to obtain $Lbl_{other}$, as shown in the table that without the domain adaptation, $Lbl_{other}$ would very much resemble random guessing. When using $Lbl_{orig}$ only, i.e. using the methods in 4.2.1 and 4.2.2 and their combination with the baseline method, the performance didn't show improvement in most cases as compared to the baselines.\par
In deception recognition, \par
\centerline{Table1. UAR in deception recognition}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|}
\hline
& Using $Lbl_{orig}$ & Using $Lbl_{other}$ & Using fusion of\\
& only & only & the 2 labels\\
\hline
Baselines & 0.6964 & \multirow{4}{*}{ } & 0.7202 \\
\cline{1-2}
\cline{4-4}
RANSAC & (a) 0.6964\tnote{1} & 0.6081 & (a) 0.7202 \\
& (b) 0.6964 & & (b) 0.7193 \\
& (c) 0.7026 & & (c) 0.7096 \\
\cline{1-2}
\cline{4-4}
Transform & 0.6717 & *w/o GFK: & 0.6990 \\
labels & & 0.5275 & \\
\cline{1-2}
\cline{4-4}
Combined\tnote{2} & (a)  & & (a) 0.7005 \\
& (b)  & & (b) 0.7005\\
& (c) 0.6933 & & (c) 0.7059\\
\hline
\end{tabular}
\begin{tablenotes}
\item[1] Listed in the table are results of the three strategies we applied. The strategies are: (a) add 5 percent of unlabeled data in each iteration; (b) add 5 percent of unlabeled data in each iteration; (c) add all of the unlabeled data in one iteration. Please refer to section 4.2.1 for more details.
\item[2] In the combined method, as stated in 4.2.3, the three methods above were integrated by selecting the classifier that performed the best on the development set in each fold. The said classifier was then used in the fusion. The identifier (a), (b), (c) indicates which RANSAC strategy was used.
\end{tablenotes}
\end{threeparttable}

\medskip

\centerline{Table2. Spearman's correlation in sincerity evaluation}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|}
\hline
& Using $Lbl_{orig}$ & Using $Lbl_{other}$ & Using fusion of\\
& only & only & the 2 labels\\
\hline
Baselines & 0.4651 (0.4158\tnote{1}) & \multirow{4}{*}{ } & 0.4853 (0.4784) \\
\cline{1-2}
\cline{4-4}
RANSAC & 0.4645\tnote{2} (0.3366) & 0.1867 (0.1755) & 0.4850 (0.4804) \\
\cline{1-2}
\cline{4-4}
Transform & 0.4344 (0.2936) & *w/o GFK: & 0.4937 (0.4818) \\
labels & & 0.0084 (0.0101) & \\
\cline{1-2}
\cline{4-4}
Combined\tnote{3} & 0.4590 (0.3108) & & 0.4896 (0.4738) \\
\hline
\end{tabular}
\begin{tablenotes}
\item[1] To conduct significance tests, Pearson's correlation was also computed and is shown in parentheses in the table.
\item[2] Between the two strategies we applied, only the best result is shown in the table. The strategy that generated the best result was then used in the fusion, while the other was not. Both two results are as follows: (a) add one cluster of the unlabeled data in each iteration: 0.4645; (b) add all of the unlabeled data in one iteration: 0.4351.
\item[3] In the combined method, as stated in 4.2.3, the three methods above were integrated by selecting the regressor that performed the best on the development set in each fold. The said regressor was then used in the fusion.
\end{tablenotes}
\end{threeparttable}

\section{Discussion}

\section{Conclusion}
\cite{grosz2015assessing}

\footnotesize{

%\vfill\pagebreak
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

}
\end{document}
